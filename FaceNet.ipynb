{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZIfzbKRIZLa",
        "outputId": "1fb928ac-ee56-4468-87d7-d70c1ea63b4b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP2VGHeMbdhK"
      },
      "source": [
        "!cp '/content/drive/MyDrive/FaceNet/facenet_keras.h5' facenet_keras.h5\n",
        "!cp '/content/drive/MyDrive/FaceNet/archive.zip' archive.zip\n",
        "!cp -r '/content/drive/MyDrive/test' test\n",
        "!cp -r '/content/drive/MyDrive/nonsense' nonsense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAy2JghcfrRb"
      },
      "source": [
        "!unzip archive.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPRYpBdObVCZ",
        "outputId": "2e9436c1-1c07-4581-ad87-2a7aad6da50d"
      },
      "source": [
        "!sudo pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07EMWha-d-Va"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isdir\n",
        "from PIL import Image\n",
        "from numpy import asarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZkoYrQOQjt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3190a0-f639-49b4-c454-40f3522d0abe"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from numpy import load, expand_dims"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLQapCukdRQ2",
        "outputId": "97892368-b42d-4680-a6b0-192d41e55c1e"
      },
      "source": [
        "# load pre-trained FaceNet model\n",
        "from keras.models import load_model\n",
        "facenet_model = load_model('facenet_keras.h5')\n",
        "print(facenet_model.inputs)\n",
        "print(facenet_model.outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
            "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg4lxIYWiigA"
      },
      "source": [
        "def image_preprocessing(filename):\n",
        "  image = Image.open(filename)\n",
        "  image = image.convert('RGB')  # input color of FaceNet\n",
        "  image = image.resize((160,160)) # input dimension of FaceNet\n",
        "  face_arr = asarray(image)\n",
        "  return face_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yeg6nE9jbH7"
      },
      "source": [
        "def load_marvel_face(subdir):\n",
        "  face = list()\n",
        "  for file in listdir(subdir):\n",
        "    img_path = subdir + file\n",
        "    face_arr = image_preprocessing(img_path)\n",
        "    face.append(face_arr)\n",
        "  return face"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XqbjOlCmG0w"
      },
      "source": [
        "def load_dataset(dir):\n",
        "  print(\"Loading the dataset...\")\n",
        "  X, y = list(), list()\n",
        "  for subdir in listdir(dir):\n",
        "    path = dir + '/' + subdir + '/'\n",
        "    if not isdir(path):\n",
        "      print(\"Image path not found...\")\n",
        "      continue\n",
        "    marvel_face = load_marvel_face(path)\n",
        "    marvel_name = [subdir for _ in range(len(marvel_face))]\n",
        "    print(\"Loaded {} images of {}\".format(len(marvel_face), subdir))\n",
        "    X.extend(marvel_face)\n",
        "    y.extend(marvel_name)\n",
        "  print(\"Successfully loaded the dataset...\")\n",
        "  return asarray(X), asarray(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwzyb7tWC4gD"
      },
      "source": [
        "def face_embedding(model,face):\n",
        "  face = face.astype('float32')\n",
        "  mean, std = face.mean(), face.std()\n",
        "  face = (face-mean)/std\n",
        "  per = expand_dims(face, axis=0)\n",
        "  yhat = model.predict(per)\n",
        "  return yhat[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Ld7-HvSfjx"
      },
      "source": [
        "def get_prediction(data):\n",
        "  output = []\n",
        "  for i in range(data.shape[0]):\n",
        "    per = expand_dims(data[i], axis = 0)\n",
        "    if max(my_model.predict_proba(per)[0])<0.8: # if class prob less than threshold of 0.4, then face is unknown\n",
        "      output.append(5)\n",
        "    else:\n",
        "      pred = my_model.predict(per)\n",
        "      output.append(pred[0])\n",
        "  return output\n",
        "def inverse_prediction(prediction):\n",
        "  result = []\n",
        "  for i in range(len(prediction)):\n",
        "    if prediction[i] == 5:\n",
        "      result.append(\"unknown\")      \n",
        "    else:\n",
        "      hero = labeller.inverse_transform(prediction[i])\n",
        "      result.append(hero)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clxWdsIHoZAF",
        "outputId": "9243dabe-bb1c-4069-842a-7293f31e8f7d"
      },
      "source": [
        "# Load data from dataset\n",
        "avengers_face, avengers_name = load_dataset('cropped_images') \n",
        "print(avengers_face.shape, avengers_name.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the dataset...\n",
            "Loaded 66 images of mark_ruffalo\n",
            "Loaded 53 images of chris_hemsworth\n",
            "Loaded 54 images of scarlett_johansson\n",
            "Loaded 51 images of robert_downey_jr\n",
            "Loaded 50 images of chris_evans\n",
            "Successfully loaded the dataset...\n",
            "(274, 160, 160, 3) (274,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsmubzqHD742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c895e2b-6268-4d40-9dca-70fcdf7352ce"
      },
      "source": [
        "# Obtain face embeddings using FaceNet Model on dataset\n",
        "new_avengers = list()\n",
        "for face in avengers_face:\n",
        "  embed = face_embedding(facenet_model,face)\n",
        "  new_avengers.append(embed)\n",
        "asarray(new_avengers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e149e710> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149e710>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e149e710> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149e710>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e149e320> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149e320>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e149e320> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149e320>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e149e4d0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149e4d0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e149e4d0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149e4d0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e149eb00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149eb00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e149eb00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149eb00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e149ec20> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149ec20>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e149ec20> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149ec20>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e149ed40> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149ed40>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e149ed40> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149ed40>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e149ee60> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149ee60>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e149ee60> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149ee60>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e149ef80> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149ef80>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e149ef80> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e149ef80>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e13770e0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e13770e0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e13770e0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e13770e0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e1377200> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377200>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e1377200> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377200>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e1377320> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377320>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e1377320> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377320>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e1377440> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377440>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e1377440> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377440>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e1377560> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377560>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e1377560> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377560>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e1377680> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377680>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e1377680> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377680>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e13777a0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e13777a0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e13777a0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e13777a0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e1377830> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377830>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e1377830> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377830>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e13779e0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e13779e0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e13779e0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e13779e0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e1377b00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377b00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e1377b00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377b00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e1377c20> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377c20>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e1377c20> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e1377c20>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e2b0fb00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e2b0fb00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e2b0fb00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e2b0fb00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7ff7e2b0f830> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e2b0f830>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7ff7e2b0f830> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7ff7e2b0f830>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.2537611 , -1.0822445 ,  0.620365  , ...,  1.4066731 ,\n",
              "        -0.13999994, -1.3223813 ],\n",
              "       [ 0.55559886, -0.5058398 ,  0.6943772 , ...,  0.46842912,\n",
              "        -0.4281253 , -0.77652323],\n",
              "       [ 0.5699115 , -0.04720436,  0.7755244 , ...,  0.5880779 ,\n",
              "        -0.03605543, -0.8967809 ],\n",
              "       ...,\n",
              "       [-0.5789609 ,  0.9173926 , -0.4548461 , ..., -0.89618623,\n",
              "         2.5931993 , -0.07359681],\n",
              "       [ 0.2548324 ,  0.8861177 , -0.5750268 , ..., -0.642449  ,\n",
              "         2.1674335 , -0.3172723 ],\n",
              "       [-0.04740684,  1.0700345 ,  0.8628357 , ..., -0.7687462 ,\n",
              "         0.95541435,  1.2531831 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEyUviR1RmFA"
      },
      "source": [
        "# Transformers:\n",
        "#Normalize the training data\n",
        "normalize = Normalizer(norm='l2')\n",
        "#Label encode the Target values, here, names of the marvel heroes\n",
        "labeller = LabelEncoder()\n",
        "new_avengers = normalize.transform(new_avengers)\n",
        "avengers_name = labeller.fit_transform(avengers_name) \n",
        "oversample = SMOTE(sampling_strategy = 'not majority', random_state = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxhi2zqkFjz6",
        "outputId": "e71e7563-1764-4581-fe4c-43cee3e109a8"
      },
      "source": [
        "print(new_avengers.shape, len(avengers_name))\n",
        "print(new_avengers[1])\n",
        "print(avengers_name[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(274, 128) 274\n",
            "[ 0.05046561 -0.04594594  0.06307098 -0.07250116 -0.10203902  0.00067759\n",
            "  0.16923039 -0.07576329  0.03182524  0.01535801 -0.0904481  -0.10881097\n",
            " -0.10275429 -0.15165363 -0.10564321  0.07248892 -0.20822802 -0.01403647\n",
            " -0.01755909 -0.15612786  0.12070401 -0.03375864 -0.03049926  0.06357411\n",
            "  0.06833943 -0.01600829 -0.135281   -0.0754101  -0.00910086 -0.02287504\n",
            "  0.01776128 -0.03185196  0.06187846  0.00613883  0.00791412 -0.13192701\n",
            " -0.02725204  0.00097572 -0.02665826  0.00114756  0.0679507   0.0758658\n",
            "  0.17258589  0.08556296 -0.02836961  0.04105192  0.05169252  0.13170515\n",
            " -0.2038522  -0.10904858  0.03865273 -0.13552909 -0.07071769  0.01699245\n",
            "  0.01476816 -0.01090955  0.00407652  0.08602078 -0.04200199  0.03531005\n",
            "  0.07031708 -0.11831962 -0.11623927 -0.08359942 -0.0265741   0.04690154\n",
            " -0.06348915  0.07061914  0.02669872 -0.10229507  0.00354535 -0.01099752\n",
            " -0.08753023 -0.05835701 -0.1574461   0.03186925 -0.01211626  0.14925171\n",
            "  0.08946069 -0.00471347  0.04834936 -0.10181343  0.00661481 -0.07213761\n",
            " -0.11319408 -0.21627128  0.1291525  -0.09925042  0.08312535  0.0430394\n",
            "  0.01967938 -0.1425788   0.07496789  0.11238684 -0.02860051 -0.02937063\n",
            " -0.08361919 -0.01762966 -0.01935636  0.02940444  0.07892776 -0.15947045\n",
            " -0.00114672  0.0675126  -0.13653778 -0.14334024 -0.12420093 -0.09307486\n",
            " -0.14120713  0.12731662  0.0429672  -0.09811075  0.15613133  0.00225508\n",
            " -0.14685325 -0.03241897  0.01519066 -0.04422965 -0.08476845  0.0118048\n",
            " -0.00479437  0.05566164 -0.0706722   0.23667967 -0.0090762   0.04254789\n",
            " -0.03888705 -0.07053239]\n",
            "[2 2 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4eg2xwPecAc",
        "outputId": "b853f7c5-570c-48d6-8d69-fc141899df76"
      },
      "source": [
        "# Use SMOTE to oversample the imbalanced dataset\n",
        "X, y = oversample.fit_resample(new_avengers, avengers_name)\n",
        "print(X.shape, len(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(330, 128) 330\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjyqWGW7TCib",
        "outputId": "30d175b4-3428-4c08-b79f-28209d7e7aa7"
      },
      "source": [
        "# Divide dataset into train and test set in 80:20 ratio\n",
        "train_face, test_face, train_name, test_name = train_test_split(X, y, test_size = 0.2, random_state = 3)\n",
        "print(train_face.shape, train_name.shape)\n",
        "print(test_face.shape, test_name.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(264, 128) (264,)\n",
            "(66, 128) (66,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFqItMTITyN-",
        "outputId": "681956b0-f7bd-467c-8cd2-ae9c5cd873bf"
      },
      "source": [
        "# Create Model for face classification\n",
        "my_model = SVC(kernel = 'linear', probability = True, verbose =  True)\n",
        "my_model.fit(train_face, train_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
              "    verbose=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYs5YK5IUibY"
      },
      "source": [
        "yhat_train = get_prediction(train_face)\n",
        "yhat_test = get_prediction(test_face)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1El7OeshU-ZZ",
        "outputId": "8031a49c-c9a4-4bbe-c3d8-75edb6b07df0"
      },
      "source": [
        "print(yhat_train[:5])\n",
        "print(train_name[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 1, 2, 2, 2]\n",
            "[4 1 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxL9S0YUVHXR",
        "outputId": "1408850b-d61f-424d-b1f0-d02932cadeec"
      },
      "source": [
        "train_accuracy = accuracy_score(train_name, yhat_train)\n",
        "test_accuracy = accuracy_score(test_name, yhat_test)\n",
        "print(\"Training accuracy of the model is {0} %.\".format(train_accuracy*100))\n",
        "print(\"Testing accuracy of the model is {0} %.\".format(test_accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy of the model is 100.0 %.\n",
            "Testing accuracy of the model is 100.0 %.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5db_4OIyRBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d31814-9151-4240-fd52-9e5e5c228682"
      },
      "source": [
        "test_img = image_preprocessing('nonsense/4.jpg')\n",
        "test_face = face_embedding(facenet_model, test_img)\n",
        "test = expand_dims(test_face, axis=0)\n",
        "prob = my_model.predict_proba(test)\n",
        "print(prob)\n",
        "pred = get_prediction(expand_dims(test_face, axis=0))\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7.99171243e-05 6.39560746e-07 1.38311488e-03 9.98528624e-01\n",
            "  7.70417016e-06]]\n",
            "[3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SJacPslkVG5",
        "outputId": "5bacf448-e831-4d8c-aea0-dbf3e25e052e"
      },
      "source": [
        "dir = 'test'\n",
        "result = []\n",
        "for file in listdir(dir):\n",
        "  path = dir + '/' + file\n",
        "  print(path)\n",
        "  img = image_preprocessing(path)\n",
        "  embed = face_embedding(facenet_model, img)\n",
        "  pred = get_prediction(expand_dims(embed, axis = 0))\n",
        "  result.append(pred[0])\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test/23.jpeg\n",
            "test/19.jpeg\n",
            "test/16.jpeg\n",
            "test/25.jpeg\n",
            "test/6.jpeg\n",
            "test/7.jpeg\n",
            "test/17.jpeg\n",
            "test/22.jpeg\n",
            "test/14.jpeg\n",
            "test/3.jpeg\n",
            "test/15.jpeg\n",
            "test/1.jpg\n",
            "test/11.jpg\n",
            "test/21.jpeg\n",
            "test/4.jpeg\n",
            "test/24.jpeg\n",
            "test/18.jpeg\n",
            "test/2.jpg\n",
            "test/12.jpeg\n",
            "test/9.jpeg\n",
            "test/20.jpeg\n",
            "test/8.jpeg\n",
            "test/13.jpeg\n",
            "test/5.jpeg\n",
            "test/10.jpeg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 2, 2, 4, 0, 0, 2, 4, 1, 3, 1, 3, 4, 4, 3, 4, 2, 3, 1, 0, 4, 0, 1, 3, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RufS5zJ7yJn",
        "outputId": "72471660-f29e-4450-be60-aa44be1f8142"
      },
      "source": [
        "dir = 'nonsense'\n",
        "result = []\n",
        "for file in listdir(dir):\n",
        "  path = dir + '/' + file\n",
        "  print(path)\n",
        "  img = image_preprocessing(path)\n",
        "  embed = face_embedding(facenet_model, img)\n",
        "  pred = get_prediction(expand_dims(embed, axis = 0))\n",
        "  result.append(pred[0])\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nonsense/3.jpg\n",
            "nonsense/4.jpg\n",
            "nonsense/1.jpg\n",
            "nonsense/2.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 3, 5, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI1feHJGxBhM",
        "outputId": "db24b082-249d-40b1-eec6-b68af65ce92e"
      },
      "source": [
        "face = [5,5,5,5]\n",
        "acc = accuracy_score(result, face)\n",
        "print(\"Accuracy score {0} %.\".format(acc*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score 75.0 %.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDhYQ4InoudW"
      },
      "source": [
        "# preprocess = Pipeline(steps = [('face_transform',face_arr_transform(facenet_model))])\n",
        "#                                #('normalize', Normalizer(norm = 'l2'))])\n",
        "# X, y = preprocess.fit_transform(X, y)\n",
        "# print(X.shape, len(y))\n",
        "# print(X[1])\n",
        "# print(y[:5])\n",
        "# X, y = oversample.fit_resample(X, y)\n",
        "# print(X.shape, y.shape)\n",
        "# print(X[1])\n",
        "# print(y[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFC6YBJwWvDi"
      },
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# RF_classifier = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=3, max_features=1)\n",
        "# RF_classifier.fit(train_face, train_name)\n",
        "# yhat_train = my_model.predict(train_face)\n",
        "# yhat_test = my_model.predict(test_face)\n",
        "# train_accuracy = accuracy_score(train_name, yhat_train)\n",
        "# test_accuracy = accuracy_score(test_name, yhat_test)\n",
        "# print(\"Training accuracy of the model is {0} %.\".format(train_accuracy*100))\n",
        "# print(\"Testing accuracy of the model is {0} %.\".format(test_accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoDOI6gfin6Y"
      },
      "source": [
        "# class face_arr_transform(BaseEstimator, TransformerMixin):\n",
        "#   def __init__(self, model):\n",
        "#     self.model = model\n",
        "  \n",
        "#   def fit(self, X, y = None):\n",
        "#     return self\n",
        "  \n",
        "#   def face_embedding(self, face):\n",
        "#     face = face.astype('float32')\n",
        "#     mean, std = face.mean(), face.std()\n",
        "#     face = (face-mean)/std\n",
        "#     per = expand_dims(face, axis=0)\n",
        "#     yhat = self.model.predict(per)\n",
        "#     return yhat[0]\n",
        "\n",
        "#   def transform(self, X, y = None):\n",
        "#     new_X = list()\n",
        "#     for face in X:\n",
        "#       embed = self.face_embedding(face)\n",
        "#       new_X.append(embed)\n",
        "#     return asarray(new_X), y\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}